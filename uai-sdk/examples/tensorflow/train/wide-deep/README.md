# Wide and Deep Example
This example shows how to run wide&deep on UAI Train platform. This example is based on https://github.com/tensorflow/models/tree/master/official/wide_deep.

## Setup
You should follow https://github.com/tensorflow/models/tree/master/official/wide_deep to download test data for wide and deep example.

## Intro
We modify the wide\_deep.py to run on UAI Platform. 

During our test, when batch\_size is small (e.g., 40 as default), CPU's performance is much better than GPU performance. If we increase the batch\_size to 1024, then GPU performance is much better. (Note you need to 'double' the dataset multiple times to test the performance with a larger batch\_size.

## UAI Example    
We do following modifications to the wide\_deep.py:  
                                                                                                        
1. Add UAI SDK related arguments: --data\_dir, --output\_dir, these arguments are auto generated by UAI Train Platform, see: https://github.com/ucloud/uai-sdk/blob/master/uaitrain/arch/tensorflow/uflag.py for more details                                                                
2. Modify code to use UAI arguments: use data_dir as input dir and use output\_dir instead of model\_dir as model output dir  
3. We also add some modifications to the code, please code/wide\_deep.py for more details

### How to run
We assume you fully understand how UAI Train docker image works and has already reads the DOCS here: https://docs.ucloud.cn/ai/uai-train/guide/tensorflow

1. Pack the wide and deep example code into docker using tf\_tools.py provided in https://github.com/ucloud/uai-sdk/blob/master/uaitrain\_tool/tf/
2. Run the docker locally or push it into UAI Train platform to run.
  
#### The simplest CMD applied to run is 
    /data/wide_deep.py --train_data=census_data/adult.data --test_data=census_data/adult.test --batch_size=1024

#### An pack cmd example is:

    sudo python tf_tool.py pack --public_key=<UCLOUD_PUB_KEY> \ 
    --private_key=<UCLOUD_PRIV_KEY> \
    --code_path=code/ \
    --mainfile_path=wide_deep.py \
    --uhub_username=<UCLOUD_ACCOUNT> \
    --uhub_password=<UCLOUD_ACCOUNT_PASSWD> \
    --uhub_registry=<UHUB_DOCKER_REGISTRY> \
    --uhub_imagename=resnet-imagenet-tf \
    --ai_arch_v=tensorflow-1.4.0 \
    --test_data_path=<LOCAL_PATH_TO_WIDE&DEEP_DATA_FOR_TEST> \
    --test_output_path=<LOCAL_PATH_TO_OUTOUT_FOR_TEST> \
    --train_params="--train_data=census_data/adult.data --test_data=census_data/adult.test --batch_size=1024"
   
Note: 
The tfrecords should stored in LOCAL\_PATH\_TO\_WIDE&DEEP\_DATA\_FOR\_TEST in this example for loacal test

### Run Distributed Training
We provide a wide&deep implementation in code/wide\_deep\_dist.py which implements tf.contrib.learn.Experiment API to run tf.estimator.Estimator (LinearClassifier, DNNClassifier and DNNLinearCombinedClassifier are all derived from Estimator). The we can run it on distributed environment directly.You only needs a distribted training environment and a dist-config. 

A standard TF\_CONFIG (compatable with the tf.estimator.Estimator API) looks like this:

    TF_CONFIG = {
        "cluster":{
            "master":["ip0:2222"],
            "ps":["ip0:2223","ip1:2223"],
            "worker":["ip1:2222"]},
        "task":{"type":"worker","index":0},
        "environment":"cloud"
    }

You can generate TF\_CONFIG for each node in your cluster and run the training.

### Run Distributed Training On UAI Platform
UAI Train Platform can dynamicaaly deploy the training cluster and generate the TF\_CONFIG for each training node. You only need to run the training cmd as:

    /data/wide_deep_dist.py --train_data=census_data/adult.data --test_data=census_data/adult.test --batch_size=1024

For more details please see https://docs.ucloud.cn/ai/uai-train.
