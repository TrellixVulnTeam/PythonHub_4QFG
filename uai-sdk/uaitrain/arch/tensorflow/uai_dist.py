import os
import json

class UAIDistRunConfig(object):
    def __init__(self, dist_master, ps_nodes, wk_nodes, task_type, task_idx):
        self._dist_master=dist_master
        self._ps_nodes = ps_nodes
        self._wk_nodes = wk_nodes
        self._task_type = task_type
        self._task_idx = task_idx

    def parse_master(self):
        """master input format:
           host0:port0
        """
        self._master_list=self._dist_master.split(',')

    def parse_ps(self):
        """ps input format:
             host0:port0,host1:port1
        """
        self._ps_list = self._ps_nodes.split(',')

    def parse_worker(self):
        """worker input format:
             hsot0:port0,host1:port1
        """
        self._wk_list = self._wk_nodes.split(',')

    def generate_cluster(self):
        pass

""" To run tensorflow distributed training, we need:
    workers: master, worker1, worker2, worker3 ....
    ps: ps0, ps1, ps2 ...

    the chief node is determined by the 'master'
    We will generate a cluster dict specifiying the dist train cluster config
    cluster = {'master': ['master-ip:2222'],
           'ps': ['ps-ip0:2222', 'ps-ip1:2222', 'ps-ip2:2222'],
           'worker': ['worker-ip0:2222', 'worker-ip1:2222', 'worker-ip2:2222']}

    We also generate a task dict specifiying current task config
       task = {'type': 'master', 'index':0}
       task = {'type': 'ps', 'index':0}
       task = {'type': 'worker', 'index':0}

    Note: master is also a worker but acting as a chief worker

    How to run:
       Master(0.0.0.1:2222)
          --master='0.0.0.1:2222' \
          --ps_nodes='0.0.1.1:2222,0.0.1.2:22222,0.0.1.3:2222' \
          --worker_nodes='0.0.0.2:2222,0.0.0.3:2222,0.0.0.4:2222' \
          --task_type=master \
          --task_idx=0 \

       Worker(0.0.0.3:2222)
          --master='0.0.0.1:2222' \
          --ps_nodes='0.0.1.1:2222,0.0.1.2:22222,0.0.1.3:2222' \
          --worker_nodes='0.0.0.2:2222,0.0.0.3:2222,0.0.0.4:2222' \
          --task_type=worker \
          --task_idx=1 \

       PS(0.0.1.1:2222)
          --master='0.0.0.1:2222' \
          --ps_nodes='0.0.1.1:2222,0.0.1.2:22222,0.0.1.3:2222' \
          --worker_nodes='0.0.0.2:2222,0.0.0.3:2222,0.0.0.4:2222' \
          --task_type=ps \
          --task_idx=0 \

    Note: estimator.py use its own RunConfig (tensorflow.python.estimator.run_config)
          which is differnt from the one tf.contrib.learn.learn_runner.run use (tensorflow.contrib.learn.python.learn.estimators.run_config)
          Both will parse the os.environ['TF_CONFIG'] to get the distributed environment
          estimator.run_config recogonize both 'master' and 'chief'
          learn.estimators.run_config only recogonize 'master'

    You can use UAITensorFlowDistRunConfig to generate your own DistConfig
"""
class UAITensorFlowDistRunConfig(UAIDistRunConfig):
    def __init__(self, dist_master, ps_nodes, wk_nodes, task_type, task_idx):
        super(UAITensorFlowDistRunConfig, self).__init__(dist_master, ps_nodes, wk_nodes, task_type, task_idx)
    
    def generate_cluster(self):
        self.parse_master()
        self.parse_ps()
        self.parse_worker()

        cluster = {'master': self._master_list,
                   'ps': self._ps_list,
                   'worker': self._wk_list}
        task = {'type': self._task_type,
                'index': self._task_idx}
        tf_dist_conf = {'cluster': cluster,
                        'task': task, 
                        'environment': 'cloud'}
        os.environ['TF_CONFIG'] = json.dumps(tf_dist_conf)

""" UAITensorFlowDistRunConfig
    UAI TensorFlow Distributed Run Config parser
    It can parse the standard TF Dist Config generated by UAI Train system, 
      You can use this class to parse the Config and setup your own dist 
      training code.

    Please refer to example/tensorflow/train/slim example for how to use 
      UAITensorFlowDistRunConfig
"""
class UAITensorFlowDistRunConfigParser(object):
    def __init__(self):
        env = os.environ.get('TF_CONFIG')
        self.env = env

    def is_dist(self):
        if self.env is None:
            return False
        return True

    def load_cluster_config(self):
        config = json.loads(self.env)

        task_env = config.get('task', {})
        task_index = task_env.get('index')
        task_type = task_env.get('type', None)
        task_id = int(task_index)
        if task_type == "worker":
            task_id = task_id + 1

        self.task_type = str(task_type)
        self.task_id = task_id

        cluster_spec = config.get('cluster', {})
        master = cluster_spec.get('master', {})
        ps_list = cluster_spec.get('ps', {})
        wk_list = cluster_spec.get('worker', {})

        master = str(master[0])
        for i in range(len(ps_list)):
            ps_list[i] = str(ps_list[i])
        for i in range(len(wk_list)):
            wk_list[i] = str(wk_list[i])
        wk_list.insert(0, master)

        self.ps_list = ps_list
        self.wk_list = wk_list

    def is_chief(self):
        return (self.task_type == "master")

    def is_ps(self):
        return (self.task_type == "ps")

    def num_ps(self):
        return len(self.ps_list)

    def num_worker(self):
        return len(self.wk_list)

    def get_ps_list(self):
        return self.ps_list

    def get_worker_list(self):
        return self.wk_list

    def get_task_id(self):
        return self.task_id
